<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on 有趣</title>
    <link>https://www.geekgame.site/series/kubernetes/</link>
    <description>Recent content in kubernetes on 有趣</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright>
    <lastBuildDate>Mon, 05 Jul 2021 11:09:37 +0800</lastBuildDate><atom:link href="https://www.geekgame.site/series/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kubernetes 监控架构(译)</title>
      <link>https://www.geekgame.site/post/k8s/monitoring_arch/</link>
      <pubDate>Mon, 05 Jul 2021 11:09:37 +0800</pubDate>
      
      <guid>https://www.geekgame.site/post/k8s/monitoring_arch/</guid>
      <description>
        
          &lt;h1 id=&#34;概要&#34;&gt;概要&lt;/h1&gt;
&lt;p&gt;监控分为两个部分:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;核心监控流程由kubelet、资源评估器、metric-server(Heapster 精简版)以及API server 上的master metrics API 组成. 这些监控数据被系统核心组件使用,例如调度逻辑(调度器和基于系统指标的HPA) 和 开箱即用的UI组件(例如 kubectl top), 这条监控管道不适合与第三方监控系统集成.&lt;/li&gt;
&lt;li&gt;另一个监控流程用于从系统收集各种指标并将这些指标导出到用户端、HPA(自定义指标)以及通过适配器到处到 infrastore. 用户可以从众多的监控系统中进行选择,也可以不运行监控系统. Kubernetes 不附带监控管道, 但是第三方的选项是很容易被安装的. 我们希望第三方管道通常由每个节点的代理和一个集群级聚合器组成.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;该架构在本文档附录中的图表中进行了说明。&lt;/p&gt;
&lt;h1 id=&#34;介绍和目标&#34;&gt;介绍和目标&lt;/h1&gt;
&lt;p&gt;本文档为Kubernetes 提出了一个高级监控架构. 它涵盖了  Kubernetes Monitoring Architecture 文档中提到的一些问题. 特别关注有望满足大量需求的监控架构(组件以及组件之间的交互), 我们没有为实现这个架构指定任何特定的时间,也没有规划路线图.&lt;/p&gt;
&lt;h2 id=&#34;术语&#34;&gt;术语&lt;/h2&gt;
&lt;p&gt;有两种指标系统指标和服务指标, 系统指标是一般的指标,通常可以从每个监控的实体获得(例如容器和节点的CPU和内存使用情况). 服务指标是在应用代码明确定义并导出的(例如API服务器状态码为500的请求数量), 系统指标和服务指标都是从用户的容器或者系统基础组件获取(主节点组件,比如API服务器, 运行在主节点的插件pod, 和运行在用户节点的插件pod)&lt;/p&gt;
&lt;p&gt;我们把系统指标分为:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;核心指标 这些指标都是Kubernetes理解并用于其内部组件和核心业务的指标 — 例如, 用于调度的指标(包括用于资源评估、初始资源/垂直自动缩放,集群自动缩放, 和Pod水平自动缩放(不包括自定义指标)), Kube 仪表盘, 和 “kubectl top”, 截至目前, 这包括cpu 累计使用情况, 内存瞬时使用情况, pod 磁盘使用情况, 容器的磁盘使用情况.&lt;/li&gt;
&lt;li&gt;非核心指标，不被 Kubernetes 解读；我们通常假设它们包括核心指标（尽管不一定采用 Kubernetes 理解的格式）以及其他指标。
我们认为日志记录与监控是分开的，因此日志记录超出了本文档的范围。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;要求&#34;&gt;要求&lt;/h1&gt;
&lt;p&gt;监控架构应该是下面这个样子：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包括作为核心 Kubernetes 一部分的解决方案和
&lt;ul&gt;
&lt;li&gt;通过标准的主 API（今天的主指标 API）使有关节点、Pod 和容器的核心系统指标可用，从而使 Kubernetes 的核心功能不依赖于非核心组件&lt;/li&gt;
&lt;li&gt;要求 Kubelet 仅导出一组有限的指标，即核心 Kubernetes 组件正确运行所需的指标（这与#18770相关）&lt;/li&gt;
&lt;li&gt;可以扩展到至少 5000 个节点&lt;/li&gt;
&lt;li&gt;足够小，我们可以要求它的所有组件在所有部署配置中运行&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;包括一个可以提供历史数据的开箱即用的解决方案，例如支持初始资源和垂直 pod 自动缩放以及集群分析查询，这仅依赖于核心 Kubernetes&lt;/li&gt;
&lt;li&gt;允许不属于核心 Kubernetes 的第三方监控解决方案，并且可以与需要服务指标的 Horizo​​ntal Pod Autoscaler 等组件集成&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;
&lt;p&gt;我们将长期架构计划的描述分为核心指标管道和监控管道。对于每个，有必要考虑如何处理来自 master 和 minion 的每种类型的指标（核心指标、非核心指标和服务指标）&lt;/p&gt;
&lt;h2 id=&#34;核心指标管道&#34;&gt;核心指标管道&lt;/h2&gt;
&lt;p&gt;核心指标管道收集一组核心系统指标。这些指标有两个来源&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubelet，提供每个节点/pod/容器的使用信息（目前属于 Kubelet 一部分的 cAdvisor 将被精简以仅提供核心系统指标）&lt;/li&gt;
&lt;li&gt;作为 DaemonSet 运行的资源估计器，将从 Kubelet 中获取的原始使用值转换为资源估计值（调度程序使用的值用于更高级的基于使用情况的调度程序）
这些来源由我们称为metrics-server的组件抓取，它类似于当今 Heapster 的精简版。metrics-server 仅在本地存储最新值并且没有接收器。metrics-server 公开主指标 API。（此处描述的配置类似于当前&amp;quot;独立&amp;quot;模式下的 Heapster。） 发现汇总器使主指标 API 可用于外部客户端，因此从客户端的角度来看，它看起来与访问 API 服务器相同。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;核心（系统）指标在所有部署环境中都按上述方式处理。唯一容易替换的部分是资源估计器，理论上，它可以由高级用户替换。metric-server 本身也可以被替换，但它类似于替换 apiserver 本身或可能与替换controller-manager类似，但不推荐也不支持。&lt;/p&gt;
&lt;p&gt;最终，核心指标管道也可能从 Kubelet 和 Docker 守护进程本身收集指标（例如 Kubelet 的 CPU 使用率），即使它们不在容器中运行。&lt;/p&gt;
&lt;p&gt;核心指标管道故意很小，并非为第三方集成而设计。&amp;quot;成熟的&amp;quot;监控留给第三方系统，它们提供监控管道（见下一节）并且可以在 Kubernetes 上运行，而无需对上游组件进行更改。通过这种方式，我们可以消除今天的负担，即维护 Heapster 作为每个可能的指标源、接收器和功能的集成点。&lt;/p&gt;
&lt;h3 id=&#34;基础设施&#34;&gt;基础设施&lt;/h3&gt;
&lt;p&gt;我们将构建一个开源 Infrastore 组件（最有可能重用现有技术），为核心系统指标和事件的历史查询提供服务，它将从主 API 中获取。Infrastore 将公开一个或多个 API（可能只是类似 SQL 的查询——这是待定的）来处理以下用例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始资源&lt;/li&gt;
&lt;li&gt;垂直自动缩放&lt;/li&gt;
&lt;li&gt;旧定时器 API&lt;/li&gt;
&lt;li&gt;用于调试、容量规划等的决策支持查询。&lt;/li&gt;
&lt;li&gt;Kubernetes 仪表板中的使用图表
此外，它可能会收集监控指标和服务指标（至少从 Kubernetes 基础设施容器中），在接下来的部分中进行了描述。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;监控管道&#34;&gt;监控管道&lt;/h2&gt;
&lt;p&gt;如上一节所述，为核心指标构建专用指标管道的目标之一是允许单独的监控管道，该管道非常灵活，因为核心 Kubernetes 组件不需要依赖它。默认情况下，我们不会提供但我们会提供一种简单的安装方法（使用单个命令，最有可能使用 Helm）。我们在本节中描述了监控管道。&lt;/p&gt;
&lt;p&gt;监控管道收集的数据可能包含以下指标组的任何子集或超集：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;核心系统指标&lt;/li&gt;
&lt;li&gt;非核心系统指标&lt;/li&gt;
&lt;li&gt;来自用户应用程序容器的服务指标&lt;/li&gt;
&lt;li&gt;来自 Kubernetes 基础设施容器的服务指标；这些指标使用 Prometheus 检测公开
由监控解决方案决定收集哪些数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了支持基于自定义指标的HPA，监控管道的提供者还必须创建一个无状态 API 适配器，从监控管道中提取自定义指标并将它们公开给 HPA。此类 API 将是一个定义明确的版本化 API，类似于常规 API。该组件的详细设计文档将介绍如何公开或发现它的详细信息。&lt;/p&gt;
&lt;p&gt;如果希望在 Infrastore 中提供监控管道指标，则应用相同的方法。这些适配器可以是独立的组件、库或监控解决方案本身的一部分。&lt;/p&gt;
&lt;p&gt;有许多可能的节点和集群级代理组合可以构成监控管道，包括 cAdvisor + Heapster + InfluxDB（或任何其他接收器）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cAdvisor + collectd + Heapster&lt;/li&gt;
&lt;li&gt;cAdvisor + Prometheus&lt;/li&gt;
&lt;li&gt;snapd + Heapster&lt;/li&gt;
&lt;li&gt;snapd + SNAP 集群级代理&lt;/li&gt;
&lt;li&gt;Sysdig
作为示例，我们将描述与 cAdvisor + Prometheus 的潜在集成。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Prometheus 在一个节点上有以下指标来源：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;来自 cAdvisor 的核心和非核心系统指标&lt;/li&gt;
&lt;li&gt;容器通过 Prometheus 格式的 HTTP 处理程序公开的服务指标&lt;/li&gt;
&lt;li&gt;[可选] 节点导出器（Prometheus 组件）中有关节点本身的指标
所有这些都由 Prometheus 集群级代理轮询。我们可以使用 Prometheus 集群级代理作为水平 pod 自动缩放自定义指标的来源，通过使用独立 API 适配器在 Prometheus 集群级代理上的 Prometheus 查询语言端点和特定于 HPA 的 API 之间进行代理/转换。同样，适配器可用于在 Infrastore 中提供来自监控管道的指标。如果用户不需要相应的功能，则不需要任何适配器。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;安装 cAdvisor+Prometheus 的命令还应该自动设置从基础设施容器收集指标。这是可能的，因为基础设施容器的名称和感兴趣的指标是 Kubernetes 控制平面配置本身的一部分，并且因为基础设施容器以 Prometheus 格式导出它们的指标。&lt;/p&gt;
&lt;h1 id=&#34;附录-架构图&#34;&gt;附录： 架构图&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://www.geekgame.site/k8s/monitoring_architecture.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;原文&#34;&gt;原文&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/monitoring_architecture.md&#34;&gt;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/monitoring_architecture.md&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>kubernetes 架构</title>
      <link>https://www.geekgame.site/post/k8s/k8s/</link>
      <pubDate>Thu, 31 Oct 2019 15:26:32 +0800</pubDate>
      
      <guid>https://www.geekgame.site/post/k8s/k8s/</guid>
      <description>
        
          &lt;h1 id=&#34;什么是-kubernetes&#34;&gt;什么是 Kubernetes&lt;/h1&gt;
&lt;p&gt;Kubernetes(简称K8s) 是由 Google 在2014年开源的容器编排与调度管理框架，主要是为用户提供一个具有普遍意义的容器编排工具。该项目是Google内部大规模集群管理系统-Borg的一个开源版本，目前是由CNCF(Cloud Native Computing Foundation)托管项目。
Kubernetes 的主要特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;可扩展：Kubernetes 是高度可配置且可扩展的。&lt;/li&gt;
&lt;li&gt;可移植：Kubernetes 不限于特定平台，可以在各种公共或者私有云平台上运行。&lt;/li&gt;
&lt;li&gt;自动化：Kubernetes 是一个高度自动化的平台：可自动部署/回滚、自我修复、自动扩缩容。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;kubernetes-架构&#34;&gt;Kubernetes 架构&lt;/h1&gt;
&lt;p&gt;K8s 遵循服务器/客户端(C/S)架构,分为两部分master和node，其中master是服务端，是控制节点主要控制和管理整个K8s集群;node是客户端,是工作节点，主要处理来自于master的任务。K8s可以设置多master来实现高可用，但是默认情况下单个master 就可以完成所有的工作。&lt;br&gt;
master包含的组件有：kube-apiserver, etcd, kube-controller-manager, kube-scheduler, cloud-controller-manager; &lt;br&gt;
node 包含的组件有: kubelet, kube-proxy;&lt;br&gt;
&lt;img src=&#34;https://www.geekgame.site/k8s/Kubernetes-101-Architecture-Diagram-768x555.jpeg&#34; alt=&#34;带有两个Worker nodes和一个master的K8s架构图&#34;&gt;
&lt;a href=&#34;https://x-team.com/blog/introduction-kubernetes-architecture/&#34;&gt;图片来源&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;master-组件&#34;&gt;master 组件&lt;/h2&gt;
&lt;p&gt;kube-apiserver: 提供集群HTTP REST API, 是集群控制的唯一入口,提供访问控制、注册、信息存储功能, 同时也是集群内部模块之间数据交换的枢纽。 &lt;br&gt;
etcd:  兼具一致性和高可用性的键值数据库,保存 K8s 所有集群数据;&lt;br&gt;
kube-scheduler:  对K8s中的Pod资源进行监控调度，为Pod选择合适的工作节点； &lt;br&gt;
kube-controller-manager: K8s实现自动化的关键组件，是集群中所有资源的自动化控制中心；&lt;br&gt;
cloud-controller-manager: 云控制器管理器是指嵌入特定云的控制逻辑的控制平面组件,使得 K8s 可以直接利用云平台实现持久化卷、负载均衡、网络路由、DNS 解析以及横向扩展等功能。&lt;/p&gt;
&lt;h2 id=&#34;node-组件&#34;&gt;node 组件&lt;/h2&gt;
&lt;p&gt;kubelet: 负责与master节点通信，处理master下发的任务，管理节点上容器的创建、停止与删除等; &lt;br&gt;
kube-proxy: 负责K8s集群服务的通信以及负载均衡；&lt;/p&gt;
&lt;h1 id=&#34;数据流转&#34;&gt;数据流转&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://www.geekgame.site/k8s/k8s_data.png&#34; alt=&#34;K8s 数据流转&#34;&gt;
我们以 ReplicaSet 为例，讲述一下K8s的数据流转：&lt;br&gt;
0. 在集群组件一启动 kube-scheduler，kube-controller-manager，kubelet就会通过list-watch机制监听自己关心的事件；&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;API作为集群入口，接收命令请求；&lt;/li&gt;
&lt;li&gt;Deployment 控制器通过 watch 获取到 Deployment 的创建事件;&lt;/li&gt;
&lt;li&gt;Deployment 控制器创建 ReplicaSet 资源;&lt;/li&gt;
&lt;li&gt;ReplicaSet 控制器通过 watch 获取到 ReplicaSet 的创建事件;&lt;/li&gt;
&lt;li&gt;ReplicaSet 控制器创建 POD 资源；&lt;/li&gt;
&lt;li&gt;Scheduler 通过 watch 获取 Pod 创建事件以及获取未绑定Node的Pod；&lt;/li&gt;
&lt;li&gt;Scheduler 通过调度算法为 Pod 选择合适的Node；&lt;/li&gt;
&lt;li&gt;kubelet 通过 watch 获取部署在当前Node的 Pod;&lt;/li&gt;
&lt;li&gt;kubelet 通过 CRI 通知container runtime 创建 Pod；&lt;/li&gt;
&lt;li&gt;container runtime 成功创建Pod；&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;观察pod创建过程相关事件&#34;&gt;观察POD创建过程相关事件&lt;/h2&gt;
&lt;p&gt;我们可以通过kubectl get events --watch 来观察POD创建过程所产生的事件。使用watch 选项可以监听整个过程。
以下是创建nginx的过程：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 1&lt;/span&gt;➜  ~ kubectl get events --watch       
&lt;span class=&#34;ln&#34;&gt; 2&lt;/span&gt;LAST SEEN   TYPE     REASON    OBJECT                                  MESSAGE
&lt;span class=&#34;ln&#34;&gt; 3&lt;/span&gt;0s          Normal   ScalingReplicaSet   deployment/nginx                        Scaled up replica &lt;span class=&#34;nb&#34;&gt;set&lt;/span&gt; nginx-6799fc88d8 to &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt; 4&lt;/span&gt;0s          Normal   SuccessfulCreate    replicaset/nginx-6799fc88d8             Created pod: nginx-6799fc88d8-dlvsf
&lt;span class=&#34;ln&#34;&gt; 5&lt;/span&gt;0s          Normal   SuccessfulCreate    replicaset/nginx-6799fc88d8             Created pod: nginx-6799fc88d8-b7868
&lt;span class=&#34;ln&#34;&gt; 6&lt;/span&gt;0s          Normal   Scheduled           pod/nginx-6799fc88d8-dlvsf              Successfully assigned default/nginx-6799fc88d8-dlvsf to k8s-node1
&lt;span class=&#34;ln&#34;&gt; 7&lt;/span&gt;0s          Normal   Scheduled           pod/nginx-6799fc88d8-b7868              Successfully assigned default/nginx-6799fc88d8-b7868 to k8s-node1
&lt;span class=&#34;ln&#34;&gt; 8&lt;/span&gt;0s          Normal   Pulling             pod/nginx-6799fc88d8-b7868              Pulling image &lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx&amp;#34;&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt; 9&lt;/span&gt;0s          Normal   Pulling             pod/nginx-6799fc88d8-dlvsf              Pulling image &lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx&amp;#34;&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt;10&lt;/span&gt;0s          Normal   Pulled              pod/nginx-6799fc88d8-b7868              Successfully pulled image &lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx&amp;#34;&lt;/span&gt; in 14.134421496s
&lt;span class=&#34;ln&#34;&gt;11&lt;/span&gt;0s          Normal   Created             pod/nginx-6799fc88d8-b7868              Created container nginx
&lt;span class=&#34;ln&#34;&gt;12&lt;/span&gt;0s          Normal   Started             pod/nginx-6799fc88d8-b7868              Started container nginx
&lt;span class=&#34;ln&#34;&gt;13&lt;/span&gt;0s          Normal   Pulled              pod/nginx-6799fc88d8-dlvsf              Successfully pulled image &lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx&amp;#34;&lt;/span&gt; in 19.163499736s
&lt;span class=&#34;ln&#34;&gt;14&lt;/span&gt;0s          Normal   Created             pod/nginx-6799fc88d8-dlvsf              Created container nginx
&lt;span class=&#34;ln&#34;&gt;15&lt;/span&gt;0s          Normal   Started             pod/nginx-6799fc88d8-dlvsf              Started container nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;以上就是关于K8s架构以及数据流转的介绍。&lt;/p&gt;
&lt;h1 id=&#34;参考&#34;&gt;参考&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.aquasec.com/cloud-native-academy/kubernetes-101/kubernetes-architecture/&#34;&gt;Kubernetes Architecture&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://x-team.com/blog/introduction-kubernetes-architecture/&#34;&gt;INTRODUCTION TO KUBERNETES ARCHITECTURE&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://kubernetes.io/zh/docs/concepts/services-networking/service/&#34;&gt;服务&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;http://dockone.io/article/4884&#34;&gt;Kubernetes的三种外部访问方式：NodePort、LoadBalancer 和 Ingress&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;&#34;&gt;极客时间&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
